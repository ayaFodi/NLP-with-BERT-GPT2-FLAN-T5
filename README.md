# ðŸŽ“ NLP Homework 5 â€” Transformers, GPT2, and Prompt Engineering

This project was developed as part of the Natural Language Processing (NLP) course at the University of Haifa, Fall 2024.  
It demonstrates practical applications of Transformer-based models for sentiment classification, text generation, and prompt engineering using the HuggingFace ecosystem.

## ðŸ“š Overview

This assignment explores multiple aspects of modern NLP:
- Fine-tuning BERT for sentiment classification
- Fine-tuning GPT2 for text generation (positive/negative movie reviews)
- Prompt engineering with FLAN-T5 using zero-shot, few-shot, and instruction-based formats
- Comparative analysis of methods and outcomes
- Bias exploration in large language models (LLMs) like ChatGPT

## ðŸ§  Tasks and Structure

### 1. BERT Sentiment Classification
- File: `bert_classification_finetuning.py`
- Fine-tunes `bert-base-uncased` on a subset of IMDB reviews (500 samples).
- Achieved ~90% accuracy on test data.
- Training choices (e.g., learning rate, epochs) are explained in the report.

### 2. GPT-2 Review Generation
- File: `gpt_generation_finetuning.py`
- Fine-tunes two separate GPT2 models:
  - One on positive reviews
  - One on negative reviews
- Each model generates 5 reviews using the prompt: `"The movie was"`.
- Generation parameters (`top_k`, `top_p`, `temperature`, `repetition_penalty`) are optimized for style and tone.

### 3. Prompt Engineering with FLAN-T5
- File: `flan_t5_prompt_engineering.py`
- Applies three prompting strategies:
  - Zero-shot
  - Few-shot (1 positive + 1 negative example)
  - Instruction-based
- Uses the FLAN-T5-small model to classify sentiment of 50 IMDB samples.

### 4. Results
- `generated_reviews.txt`: Texts generated by the two GPT-2 models.
- `flan_t5_imdb_results.txt`: Classification results of the FLAN-T5 model under different prompting strategies.

### 5. Report
- `report_hebrew.pdf`: Hebrew report summarizing code decisions, training arguments, result analysis, and answers to theoretical and ethical questions.

## ðŸ’¾ Dataset

A random subset of 500 samples was selected from the HuggingFace IMDB dataset.
- Saved locally in the folder: `imdb_subset/`

## ðŸ§ª Running the Scripts

```bash
# Fine-tune and evaluate BERT
python bert_classification_finetuning.py imdb_subset/

# Generate reviews using GPT2
python gpt_generation_finetuning.py imdb_subset/ results/generated_reviews.txt saved_models_dir/

# Run FLAN-T5 prompt engineering
python flan_t5_prompt_engineering.py imdb_subset/ results/flan_t5_imdb_results.txt
```

## ðŸ“¦ Dependencies

- `transformers`
- `datasets`
- `torch`
- `pandas`, `numpy`
- Python 3.8+

Install all dependencies with:

```bash
pip install -r requirements.txt
```

## ðŸ” Notes

- The models were fine-tuned on a small dataset to allow fast execution, even on CPU.
- All models and results are saved in `saved_models_dir/` and `results/` respectively.
- Model checkpoints were omitted from the repository to reduce its size.

---

## ðŸ“„ Report Language

The final report is written in Hebrew and includes:
- Training arguments and justifications
- Prompt design comparisons
- Accuracy evaluations
- Bias exploration with ChatGPT

---

## ðŸ‘¥ Authors

- **Aya Fodi**  
- **Rema Hanna**

> NLP Course â€“ University of Haifa, Fall 2024
